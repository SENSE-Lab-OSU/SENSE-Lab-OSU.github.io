I"°<p>In this post, weâ€™ll describe how to download and run Spark.</p>

<h3 id="download-spark">Download Spark</h3>

<p>Basically, here is Spark <a href="http://spark.apache.org/">page</a>. You can go to download
<a href="http://spark.apache.org/downloads.html">page</a> in order to download Spark.</p>

<ul>
  <li>Choose Spark release: <code class="language-plaintext highlighter-rouge">1.6.0</code></li>
  <li>Choose a package type: <code class="language-plaintext highlighter-rouge">Pre-built for Hadoop 2.6 or later</code></li>
  <li>Choose a download type: <code class="language-plaintext highlighter-rouge">Direct Download</code></li>
</ul>

<p>Then click download Spark link, it will download Spark (size around 276 MB compressed).
If you want to download to instance, just copy the link and use <code class="language-plaintext highlighter-rouge">wget</code> to download.</p>

<h3 id="run-pyspark-on-ipython-notebook">Run PySpark on IPython notebook</h3>

<p>Assume we download Spark into Desktop directory. We first have to path to Spark environment path
into <code class="language-plaintext highlighter-rouge">.bash_profile</code>, something like the following line</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">SPARK_HOME</span><span class="o">=</span>~/Desktop/spark-1.6.0-bin-hadoop2.6
</code></pre></div></div>

<p>Simple way to run <code class="language-plaintext highlighter-rouge">pyspark</code> shell is running <code class="language-plaintext highlighter-rouge">.bin/pyspark</code> (if you are in <code class="language-plaintext highlighter-rouge">spark-1.6.0-bin-hadoop2.6</code> folder).
However, we typically run <code class="language-plaintext highlighter-rouge">pyspark</code> on IPython notebook. And it will look something like</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">IPYTHON_OPTS</span><span class="o">=</span><span class="s2">"notebook"</span> ~/Desktop/spark-1.6.0-bin-hadoop2.6/bin/pyspark
</code></pre></div></div>

<p>You can also customize Spark script as follows where we state how many cores we want to use in <code class="language-plaintext highlighter-rouge">--master local[4]</code>,
driver memory and executor-memory in <code class="language-plaintext highlighter-rouge">--driver-memory, --executor-memory</code>. Also the output size is now set to <code class="language-plaintext highlighter-rouge">spark.driver.maxResultSize=0</code> which means output can be any size. You can also set it to <code class="language-plaintext highlighter-rouge">1g</code> for maximum 1 Gb output size</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">IPYTHON_OPTS</span><span class="o">=</span><span class="s2">"notebook --ip=* --no-browser"</span> ~/spark-1.6.0-bin-hadoop2.6/bin/pyspark <span class="nt">--master</span> <span class="nb">local</span><span class="o">[</span>4] <span class="nt">--driver-memory</span> 32g <span class="nt">--executor-memory</span> 32g <span class="nt">--conf</span> spark.driver.maxResultSize<span class="o">=</span>0
</code></pre></div></div>

<h3 id="snippet">Snippet</h3>

<p>Now, weâ€™re going to run small example snippet in case youâ€™re submitting the job on cluster.
First, we can create python file name <code class="language-plaintext highlighter-rouge">spark_example.py</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkConf</span><span class="p">,</span> <span class="n">SparkContext</span>

<span class="c1"># Configure the environment                                                     
</span><span class="k">if</span> <span class="s">'SPARK_HOME'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">:</span>
    <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'SPARK_HOME'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'/home/ubuntu/spark-1.6.0-bin-hadoop2.6'</span>

<span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">().</span><span class="n">setAppName</span><span class="p">(</span><span class="s">'pubmed_open_access'</span><span class="p">).</span><span class="n">setMaster</span><span class="p">(</span><span class="s">'local[32]'</span><span class="p">)</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">ls</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">ls_rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ls</span><span class="p">,</span> <span class="n">numSlices</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">ls_out</span> <span class="o">=</span> <span class="n">ls_rdd</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">).</span><span class="n">collect</span><span class="p">()</span>
    <span class="k">print</span> <span class="s">'output!: '</span><span class="p">,</span> <span class="n">ls_out</span>
</code></pre></div></div>

<p>Here is the bash script to run the code above.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>~/spark-1.6.0-bin-hadoop2.6/bin/pyspark <span class="nt">--master</span> <span class="nb">local</span><span class="o">[</span>8] <span class="nt">--driver-memory</span> 12g <span class="nt">--executor-memory</span> 12g spark_example.py
</code></pre></div></div>
:ET