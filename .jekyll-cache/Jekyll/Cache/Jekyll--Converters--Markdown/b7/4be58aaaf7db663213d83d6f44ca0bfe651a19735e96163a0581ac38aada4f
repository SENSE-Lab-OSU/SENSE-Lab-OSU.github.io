I"Ò$<h1 id="upcoming-lab-teachings">Upcoming lab teachings</h1>

<p>Every Friday, we get together (over pizza, sometimes) for lab teachings.
On a rotating basis, each member of the lab speaks and teaches about something they know.
Anything, really. Relevant and interesting topics, good skills to know, nice Python packages,
neuroscientific princples, new findings and literature reviews‚Ä¶ whatever!</p>

<p>Get on the listserve for announcements: https://groups.google.com/forum/#!forum/kording-lab-teachings</p>

<p><strong>Summer 2020</strong></p>

<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Name</th>
      <th>Topic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Aug. 19</td>
      <td>Ben B</td>
      <td>Natural Information &amp; Intentionality</td>
    </tr>
    <tr>
      <td>Aug. 26</td>
      <td>Ben</td>
      <td>Convex optimization</td>
    </tr>
    <tr>
      <td>Sept. 2</td>
      <td>Gene</td>
      <td>Recent methods in NLP</td>
    </tr>
    <tr>
      <td>Sept. 9</td>
      <td>Ilenna</td>
      <td>A history of transhumanist thought</td>
    </tr>
    <tr>
      <td>Sept. 18</td>
      <td>Roozbeh</td>
      <td>TBD</td>
    </tr>
    <tr>
      <td>Sept. 25</td>
      <td>¬†</td>
      <td>No lab teaching due to Interlab Teatime</td>
    </tr>
    <tr>
      <td>Oct. 2</td>
      <td>Amandeep</td>
      <td>In-Memory Compute</td>
    </tr>
    <tr>
      <td>Oct. 9</td>
      <td>Justin</td>
      <td>TBD, maybe scaling laws in biology</td>
    </tr>
    <tr>
      <td>Nov. 6</td>
      <td>Nidhi</td>
      <td>TBD, maybe minimal energy consumption in neuro theory</td>
    </tr>
    <tr>
      <td>Nov. 13</td>
      <td>Tony</td>
      <td>Interpretable ML</td>
    </tr>
    <tr>
      <td>Nov. 20</td>
      <td>Richard</td>
      <td>Discussion of <a href="http://arxiv.org/abs/1108.1791">‚ÄúWhy Philosophers Should Care About Computational Complexity‚Äù</a></td>
    </tr>
  </tbody>
</table>

<p><strong>Requests and suggestions</strong></p>

<ol>
  <li>Recent progress in NLP (Transformer networks, pretraining methods‚Ä¶)</li>
  <li>Graph Convolution Technique</li>
</ol>

<p><strong>Recently taught topics</strong></p>

<p><em>For inspiration. Add ones you‚Äôve done!!</em></p>

<p><strong>Fall 2018</strong></p>

<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Name</th>
      <th>Topic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Sept. 28</td>
      <td>Ilenna</td>
      <td>Capacity of Neural Networks</td>
    </tr>
    <tr>
      <td>Oct. 5</td>
      <td>Tung Pham</td>
      <td>GANs for EEG</td>
    </tr>
    <tr>
      <td>Oct. 12</td>
      <td>Ben</td>
      <td>GPUs ‚Äì beneath the heatsink <a href="https://github.com/benlansdell/gpu_samples">Slides</a></td>
    </tr>
    <tr>
      <td>Oct. 19</td>
      <td>Rachit</td>
      <td>Graph Convolution Networks</td>
    </tr>
    <tr>
      <td>Oct. 26</td>
      <td>Tony</td>
      <td>Docker for science</td>
    </tr>
    <tr>
      <td>Nov. 2</td>
      <td>Titipat</td>
      <td>AllenNLP library and a little bit of Pytorch</td>
    </tr>
    <tr>
      <td>Nov. 9</td>
      <td>Roozbeh</td>
      <td>Multiple Hypothesis Testing</td>
    </tr>
    <tr>
      <td>Nov. 16</td>
      <td>David</td>
      <td>Reinforcement learning and catastrophic forgetting</td>
    </tr>
    <tr>
      <td>Dec. 3</td>
      <td>Ari</td>
      <td>Independent Component Analysis</td>
    </tr>
  </tbody>
</table>

<p><strong>Spring 2019</strong></p>

<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Name</th>
      <th>Topic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Jan. 9</td>
      <td>Netanel Ofer</td>
      <td>Automated Analysis of Interneuron Axonal Tree Morphology and Activity Patterns</td>
    </tr>
    <tr>
      <td>Jan. 18</td>
      <td>Nidhi</td>
      <td>Dynamic Time Warping</td>
    </tr>
    <tr>
      <td>Jan. 25</td>
      <td>Ben</td>
      <td>Bandit problems</td>
    </tr>
    <tr>
      <td>Feb. 11</td>
      <td>David</td>
      <td>Autoencoders &amp; Information Bottleneck</td>
    </tr>
    <tr>
      <td>Feb. 27</td>
      <td>Adrian Radillo</td>
      <td>Perfecting the research process [dropbox doc from the teaching] (https://paper.dropbox.com/doc/Kordings-lab-teaching-on-IT-for-scientists‚ÄìAYUMIhaJvifuArh1uCfm6BivAQ-wXXjZyfix7HiGu9lcroyR)</td>
    </tr>
    <tr>
      <td>Mar. 6</td>
      <td>Ari</td>
      <td>Biologically plausible backprop</td>
    </tr>
    <tr>
      <td>Mar. 13</td>
      <td>Greg Corder (http://www.corderlab.com/)</td>
      <td>emotional processing of pain in the amygdala</td>
    </tr>
    <tr>
      <td>Mar. 20</td>
      <td>Ilenna</td>
      <td>Topics in the Philosophy of Science</td>
    </tr>
    <tr>
      <td>Mar. 27</td>
      <td>Tony</td>
      <td>Code Workflow for Research</td>
    </tr>
    <tr>
      <td>May 1</td>
      <td>Edgar Dobriban</td>
      <td>Data augmentation</td>
    </tr>
    <tr>
      <td>May 15</td>
      <td>Ben Baker (Miracchi lab)</td>
      <td>Representation and information in neuroscience</td>
    </tr>
    <tr>
      <td>May 29</td>
      <td>Sebastien Tremblay (Platt Lab)</td>
      <td>The limits of neurophys and why we need your help</td>
    </tr>
    <tr>
      <td>June 5</td>
      <td>Zhihao (Princeton University)</td>
      <td>TBA</td>
    </tr>
  </tbody>
</table>

<p><strong>Fall 2019</strong></p>

<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Name</th>
      <th>Topic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>October 9</td>
      <td>David Rolnick</td>
      <td>Climate change</td>
    </tr>
    <tr>
      <td>October 16</td>
      <td>Ari Benjamin</td>
      <td>TBD (plasticity &amp; learning in the brain)</td>
    </tr>
    <tr>
      <td>October 23</td>
      <td>Ethan Blackwood</td>
      <td>Neural models of indirection and abstraction</td>
    </tr>
    <tr>
      <td>October 30</td>
      <td>Ben Lansdell</td>
      <td>Invariance and causality</td>
    </tr>
    <tr>
      <td>November 6</td>
      <td>Nidhi Seethapathi</td>
      <td>Inferring Dynamics from Data</td>
    </tr>
    <tr>
      <td>November 13</td>
      <td>Tony Liu</td>
      <td>Theory of Computation</td>
    </tr>
    <tr>
      <td>November 20</td>
      <td>Ilenna Jones</td>
      <td>Ion Channel Kinetics</td>
    </tr>
    <tr>
      <td>November 27</td>
      <td>Shaofei Wang</td>
      <td>Differentiable Structured Inference and Attention</td>
    </tr>
    <tr>
      <td>December 4</td>
      <td>Rachit Saluja</td>
      <td>Compressed sensing and deep learning</td>
    </tr>
    <tr>
      <td>December 18</td>
      <td>Titipat Achakulvisut</td>
      <td>Reinforcement Learning (introduction)</td>
    </tr>
  </tbody>
</table>

<p><strong>Spring 2020</strong></p>

<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Name</th>
      <th>Topic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Jan. 15</td>
      <td>Ari</td>
      <td>Variational Inference</td>
    </tr>
    <tr>
      <td>Jan. 22</td>
      <td>Roozbeh</td>
      <td>Why overparameterized deep networks generalize well?</td>
    </tr>
    <tr>
      <td>Jan. 29</td>
      <td>Pedro</td>
      <td>Canonical Correlation Analysis + Update on my research on dimensionality of populations of neurons</td>
    </tr>
    <tr>
      <td>Feb. 5</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>Feb. 12</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>Feb. 19</td>
      <td>Brad Wyble</td>
      <td>TBA</td>
    </tr>
    <tr>
      <td>Feb. 26</td>
      <td>Tony</td>
      <td>The deconfounder: blessing or curse?</td>
    </tr>
    <tr>
      <td>Mar. 4</td>
      <td>Jaan Altosaar</td>
      <td>Postdoc Candidate Talk</td>
    </tr>
    <tr>
      <td>Mar. 11</td>
      <td>Titipat</td>
      <td>Reinforcement Learning (policy based, actor-critic, ‚Ä¶) - continue</td>
    </tr>
    <tr>
      <td>Mar. 18</td>
      <td>Ben</td>
      <td>Convex Optimization</td>
    </tr>
    <tr>
      <td>Mar. 25</td>
      <td>Sebastien Tremblay</td>
      <td>TBA</td>
    </tr>
    <tr>
      <td>Apr. 1</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>Apr. 8</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td>Apr. 15</td>
      <td>Rachit</td>
      <td>Data Visualization</td>
    </tr>
    <tr>
      <td>Apr. 22</td>
      <td>Ben Lansdell</td>
      <td>TBA</td>
    </tr>
    <tr>
      <td>Apr. 29</td>
      <td>Ilenna</td>
      <td>TBA</td>
    </tr>
    <tr>
      <td>May 6</td>
      <td>Nachi Stern</td>
      <td>Design and learning in physical networks</td>
    </tr>
    <tr>
      <td>May 13</td>
      <td>Roozbeh</td>
      <td>TBA</td>
    </tr>
  </tbody>
</table>

<p>Older:</p>
<ol>
  <li>Generalization in neural networks (Ari)</li>
  <li>Synaptic learning rules (Ari)</li>
  <li>How to science (debugging strategies etc.) (Konrad)</li>
  <li>Reinforcement learning and causal inference (Ben)</li>
  <li>DAGs and causal inference (Ben)</li>
  <li>Neuron firing dynamics and bifurcations (Ilenna)</li>
  <li>Submodular functions (Roozbeh)</li>
  <li>Recommendation systems (Rachit)</li>
</ol>

<h1 id="previous-lab-teaching">Previous lab teaching</h1>

<ul>
  <li><a href="http://kordinglab.com/lab_teaching_2016/">2016 topics</a></li>
  <li><a href="https://github.com/KordingLab/lab_teaching_2015">2015 topics</a></li>
</ul>
:ET